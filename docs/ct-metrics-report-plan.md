# CT Metrics Integration Report Plan

## 1) Motivation & Gap
- Claim: placeholder exotic scores saturate and provide limited discrimination.
- Evidence: Mission 16/17 pilot logs show limited movement under placeholder scoring.
- Deliverable: diagnostic subsection with 2–3 log excerpts.
- Related pilot: Mission 9.5 (arrow discovery) supplies empirical arrows for CT structure and provenance scoring.

## 2) CT Metric Definitions
- Vision validity: category laws (identity + composition) and non‑degenerate size.
- Plan fidelity: functor preserves composition; morphism coverage rate.
- Adapt coherence: naturality residual mean + per‑morphism residuals.
- Evidence grounding: ratio of plan edits with evidence artifacts.
- Deliverable: table of metrics with ranges/units and expected behavior.

## 3) Metric Computation Pipeline
- Flow: pattern-id → ct-template → vision → plan functor → adapt → metrics.
- Evidence attachment: artifacts linked to plan/adapt updates.
- Deliverable: pipeline diagram + minimal log schema (EDN).
- Add arrow graph inputs when available (discovered arrows → mission decomposition cues).
  - Pilot note: Arrow MVP produced 58 arrows with coarse word-class normalization (k=2, tau=0.6).

## 4) Experimental Protocol
- Head-to-head: CT-scored vs non-CT scorer, identical seeds/budgets.
- Windowing: fixed eval windows; report deltas and gating.
- Deliverable: reproducible commands + run profile.

## 5) Results & Interpretation
- Compare score distributions and selection trajectories.
- Include representative CA images for cooler regimes.
- Deliverable: figures + concise interpretation.

## 6) Limits & Next Steps
- Known risks: template regret, plan laundering, window conflicts.
- Future work: tighten thresholds, enrich templates, real evidence stores.
- Next pilot: fold Mission 9.5 arrow discovery outputs into CT metrics calibration.
